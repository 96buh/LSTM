{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料讀取與整合\n",
    "* 數據路徑與對應標籤（正常=0 / 異常=1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from settings import *\n",
    "import scienceplots  \n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 圖片要儲存到哪個資料夾\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "def process_file(file_path, label):\n",
    "    \"\"\"\n",
    "    讀取 CSV 檔案並切分為 CHUNK_SIZE 的小段。\n",
    "    :param file_path: 檔案路徑\n",
    "    :param label: 該檔案的標籤 (0: 正常, 1: 異常)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 假設有 'current', 'voltage', 'power' 3個columns\n",
    "    current = df['current'].values  # shape: (N,)\n",
    "    voltage = df['voltage'].values\n",
    "    power   = df['power'].values\n",
    "    \n",
    "    # 組合特徵維度: (N, 3)\n",
    "    sequence = np.column_stack((current, voltage, power))\n",
    "    seq_len = sequence.shape[0]\n",
    "    \n",
    "    # 將數據切分為 CHUNK_SIZE 的小段\n",
    "    num_chunks = seq_len // MAX_SEQ_LEN\n",
    "    for i in range(num_chunks):\n",
    "        start = i * MAX_SEQ_LEN\n",
    "        end = start + MAX_SEQ_LEN\n",
    "        chunk = sequence[start:end]\n",
    "        all_sequences.append(chunk)\n",
    "        all_labels.append(label)\n",
    "\n",
    "\n",
    "# === 讀取 normal 資料夾的 CSV，標籤=0 ===\n",
    "for filename in os.listdir(NORMAL_DIR):\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        file_path = os.path.join(NORMAL_DIR, filename)\n",
    "        process_file(file_path, label=0)\n",
    "\n",
    "# === 讀取 abnormal 資料夾的 CSV，標籤=1 ===\n",
    "for filename in os.listdir(ABNORMAL_DIR):\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        file_path = os.path.join(ABNORMAL_DIR, filename)\n",
    "        process_file(file_path, label=1)\n",
    "        \n",
    "# 轉為 numpy array\n",
    "all_sequences = np.array(all_sequences, dtype=np.float32)  # shape: (num_samples, 30, 3)\n",
    "all_labels = np.array(all_labels, dtype=np.int64)          # shape: (num_samples,)\n",
    "\n",
    "print(\"all_sequences shape:\", all_sequences.shape)\n",
    "print(\"all_labels shape:\", all_labels.shape)\n",
    "print(\"Number of normal samples:\", np.sum(all_labels == 0))\n",
    "print(\"Number of abnormal samples:\", np.sum(all_labels == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自訂 Dataset 與 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargingDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences  # shape: (num_samples, 30, 3)\n",
    "        self.labels = labels        # shape: (num_samples,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.sequences[idx]    # (30, 3)\n",
    "        y = self.labels[idx]       # 0 or 1\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM模型\n",
    "輸入(voltage, current, power) \n",
    "輸出(normal / abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes=2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim) 可能在 GPU (cuda) 或 CPU\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # h0: , c0: Initialize cell state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # out: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        out = out[:, -1, :]  # 取最後時刻的輸出 (batch_size, hidden_dim)\n",
    "        out = self.fc(out)   # (batch_size, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChargingDataset(all_sequences, all_labels)\n",
    "kfold = StratifiedKFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            running_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = correct / total if total > 0 else 0\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folds_metrics = []\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(kfold.split(all_sequences, all_labels)):\n",
    "    print(f\"\\n=== Fold {fold_idx+1} / {kfold.n_splits} ===\")\n",
    "\n",
    "    # 初始化 TensorBoard 的 SummaryWriter，將每個 fold 的日誌寫入不同的子目錄\n",
    "    writer = SummaryWriter(log_dir=os.path.join(RESULT_DIR, f\"fold_{fold_idx}\"))\n",
    "    \n",
    "    # -- 建立當前fold的 train / test data --\n",
    "    train_sequences = all_sequences[train_indices]\n",
    "    train_labels    = all_labels[train_indices]\n",
    "    test_sequences  = all_sequences[test_indices]\n",
    "    test_labels     = all_labels[test_indices]\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = ChargingDataset(train_sequences, train_labels)\n",
    "    test_dataset  = ChargingDataset(test_sequences,  test_labels)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 每個 fold 需要新建一個 LSTM 模型並重新訓練\n",
    "    model = LSTMClassifier(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, NUM_CLASSES).to(device)\n",
    "\n",
    "    # class_weights = torch.tensor([1.0, 1.2], device=device)  # 假設異常類別重要性更高\n",
    "    # class_weights = torch.tensor([1.17, 0.83], device=device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # 用於記錄歷次 epoch 的 loss/acc\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    test_precision_list = []\n",
    "    test_recall_list = []\n",
    "    test_f1_list = []\n",
    "    \n",
    "    # === 開始訓練 (num_epochs) ===\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        # epoch的 train loss/acc\n",
    "        epoch_train_loss = running_loss / total\n",
    "        epoch_train_acc = correct / total if total>0 else 0\n",
    "\n",
    "        \n",
    "        # 接著在同一epoch，做一次 test_loader 的評估\n",
    "        epoch_test_loss, epoch_test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        # 在每個 epoch 結束時，計算 Precision, Recall, F1-score\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                outputs = model(x_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                preds.extend(predicted.cpu().numpy())\n",
    "                trues.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        test_precision = precision_score(trues, preds, zero_division=0)\n",
    "        test_recall = recall_score(trues, preds, zero_division=0)\n",
    "        test_f1 = f1_score(trues, preds, zero_division=0)\n",
    "\n",
    "        test_precision_list.append(test_precision)\n",
    "        test_recall_list.append(test_recall)\n",
    "        test_f1_list.append(test_f1)\n",
    "\n",
    "        train_loss_list.append(epoch_train_loss)\n",
    "        train_acc_list.append(epoch_train_acc)\n",
    "        test_loss_list.append(epoch_test_loss)\n",
    "        test_acc_list.append(epoch_test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"TrainLoss: {epoch_train_loss:.4f}, TrainAcc: {epoch_train_acc:.4f} | \"\n",
    "              f\"TestLoss: {epoch_test_loss:.4f}, TestAcc: {epoch_test_acc:.4f} | \"\n",
    "              f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "        # 將指標記錄到 TensorBoard\n",
    "        writer.add_scalar('Loss/Train', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Test', epoch_test_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/Test', epoch_test_acc, epoch)\n",
    "        writer.add_scalar('Precision/Test', test_precision, epoch)\n",
    "        writer.add_scalar('Recall/Test', test_recall, epoch)\n",
    "        writer.add_scalar('F1_Score/Test', test_f1, epoch)\n",
    "    \n",
    "    dummy_input = torch.randn(1, MAX_SEQ_LEN, INPUT_DIM).to(device)\n",
    "    writer.add_graph(model, dummy_input)\n",
    "                                                        \n",
    "    writer.close()\n",
    "\n",
    "    epochs_range = range(1, NUM_EPOCHS+1)\n",
    "    # 新增：每個 fold 訓練結束後，建立一個 DataFrame 來儲存該 fold 的指標\n",
    "    fold_metrics_df = pd.DataFrame({\n",
    "        'Epoch': epochs_range,\n",
    "        'Train Loss': train_loss_list,\n",
    "        'Test Loss': test_loss_list,\n",
    "        'Train Accuracy': train_acc_list,\n",
    "        'Test Accuracy': test_acc_list,\n",
    "        'Test Precision': test_precision_list,\n",
    "        'Test Recall': test_recall_list,\n",
    "        'Test F1-score': test_f1_list\n",
    "    })\n",
    "    fold_metrics_df['Fold'] = fold_idx \n",
    "    all_folds_metrics.append(fold_metrics_df)\n",
    "\n",
    "    # === 混淆矩陣 ===\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            trues.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(trues, preds, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    disp.plot(values_format='d', cmap='Blues')\n",
    "    plt.title(f\"Fold {fold_idx} - Confusion Matrix\")\n",
    "    \n",
    "    # 儲存到 result/ 目錄\n",
    "    fold_cm_path = os.path.join(RESULT_DIR, f\"fold_{fold_idx}_cm.pdf\")\n",
    "    plt.savefig(fold_cm_path, bbox_inches='tight')\n",
    "    fold_cm_path = os.path.join(RESULT_DIR, f\"fold_{fold_idx}_cm.svg\")\n",
    "    plt.savefig(fold_cm_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 儲存模型\n",
    "    model_save_path = os.path.join(RESULT_DIR, f\"fold_{fold_idx}_model.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path) # 新增：儲存模型狀態字典\n",
    "    print(f\"Model for fold {fold_idx} saved to {model_save_path}\") # 新增：印出模型儲存訊息\n",
    "\n",
    "# 所有 fold 循環結束後，將 all_folds_metrics 列表中的 DataFrame 合併\n",
    "all_metrics_df = pd.concat(all_folds_metrics, ignore_index=True)\n",
    "\n",
    "# 將合併後的 DataFrame 儲存到 CSV 檔案\n",
    "all_metrics_csv_path = os.path.join(RESULT_DIR, \"all_folds_metrics.csv\")\n",
    "all_metrics_df.to_csv(all_metrics_csv_path, index=False)\n",
    "print(f\"All folds metrics saved to {all_metrics_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 圖表繪製"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = all_metrics_df['Fold'].unique()\n",
    "\n",
    "for fold in folds:\n",
    "    fold_data = all_metrics_df[all_metrics_df['Fold'] == fold]\n",
    "\n",
    "    # with plt.style.context([\"science\"]):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "    fig.suptitle(f'Fold {fold} Metrics', fontsize=16)\n",
    "\n",
    "    # Train/Test Loss\n",
    "    axes[0].plot(fold_data['Epoch'], fold_data['Train Loss'], label='Train Loss')\n",
    "    axes[0].plot(fold_data['Epoch'], fold_data['Test Loss'], label='Test Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Train/Test Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Train/Test Accuracy\n",
    "    axes[1].plot(fold_data['Epoch'], fold_data['Train Accuracy'], label='Train Accuracy')\n",
    "    axes[1].plot(fold_data['Epoch'], fold_data['Test Accuracy'], label='Test Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Train/Test Accuracy')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Precision\n",
    "    axes[2].plot(fold_data['Epoch'], fold_data['Test Precision'], label='Precision') \n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].set_title('Precision')\n",
    "    axes[2].legend()\n",
    "\n",
    "    # Recall\n",
    "    axes[3].plot(fold_data['Epoch'], fold_data['Test Recall'], label='Recall') \n",
    "    axes[3].set_xlabel('Epoch')\n",
    "    axes[3].set_ylabel('Recall')\n",
    "    axes[3].set_title('Recall')\n",
    "    axes[3].legend()\n",
    "\n",
    "    # F1-score\n",
    "    axes[4].plot(fold_data['Epoch'], fold_data['Test F1-score'], label='F1-score') \n",
    "    axes[4].set_xlabel('Epoch')\n",
    "    axes[4].set_ylabel('F1-Score')\n",
    "    axes[4].set_title('F1-Score')\n",
    "    axes[4].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fold_plot_path = os.path.join(RESULT_DIR, f\"fold_{fold}_all_metrics.pdf\")\n",
    "    plt.savefig(fold_plot_path, bbox_inches='tight')\n",
    "    fold_plot_path = os.path.join(RESULT_DIR, f\"fold_{fold}_all_metrics.svg\")\n",
    "    plt.savefig(fold_plot_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
